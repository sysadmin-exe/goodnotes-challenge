"""
Load testing script for http-echo services using Locust.
Generates traffic to a configurable list of URLs.
"""

import os
import random
import json
import time
from datetime import datetime
from locust import HttpUser, task, between, events
from locust.env import Environment
from locust.stats import stats_printer, stats_history, RequestStats


# Default URLs - can be overridden via --urls argument
DEFAULT_URLS = [
    {"name": "foo", "url": "http://foo.localhost", "expected": "foo"},
    {"name": "bar", "url": "http://bar.localhost", "expected": "bar"},
]

# Global variable to hold URLs (set from main)
TEST_URLS = []


class EchoUser(HttpUser):
    """User that randomly hits configured endpoints."""
    
    # Set a dummy host - we'll use absolute URLs for requests
    host = "http://localhost"
    wait_time = between(0.1, 0.5)  # Random wait between 100ms and 500ms
    
    def on_start(self):
        """Called when a simulated user starts."""
        self.endpoints = TEST_URLS
    
    @task
    def hit_random_endpoint(self):
        """Hit a random endpoint from the configured list."""
        endpoint = random.choice(self.endpoints)
        
        # Override the host for this request
        with self.client.get(
            endpoint['url'],
            name=f"/{endpoint['name']}",
            catch_response=True
        ) as response:
            if response.status_code == 200:
                if endpoint.get('expected'):
                    if endpoint['expected'] in response.text:
                        response.success()
                    else:
                        response.failure(f"Expected '{endpoint['expected']}' in response")
                else:
                    response.success()
            else:
                response.failure(f"Status code: {response.status_code}")


def generate_markdown_report(stats: RequestStats, duration: float, thresholds: dict) -> str:
    """Generate a Markdown report from Locust stats."""
    
    total_requests = stats.total.num_requests
    total_failures = stats.total.num_failures
    failure_rate = (total_failures / total_requests * 100) if total_requests > 0 else 0
    rps = total_requests / duration if duration > 0 else 0
    
    md = f"## üöÄ Load Test Results\n\n"
    md += f"### Summary\n\n"
    md += f"| Metric | Value |\n"
    md += f"|--------|-------|\n"
    md += f"| Total Requests | {total_requests} |\n"
    md += f"| Failed Requests | {total_failures} ({failure_rate:.2f}%) |\n"
    md += f"| Requests/sec | {rps:.2f} |\n"
    md += f"| Test Duration | {duration:.2f}s |\n\n"
    
    md += f"### HTTP Request Duration\n\n"
    md += f"| Percentile | Duration |\n"
    md += f"|------------|----------|\n"
    md += f"| Average | {stats.total.avg_response_time:.2f}ms |\n"
    md += f"| Minimum | {stats.total.min_response_time:.2f}ms |\n"
    md += f"| Maximum | {stats.total.max_response_time:.2f}ms |\n"
    md += f"| Median (p50) | {stats.total.get_response_time_percentile(0.50):.2f}ms |\n"
    md += f"| p90 | {stats.total.get_response_time_percentile(0.90):.2f}ms |\n"
    md += f"| p95 | {stats.total.get_response_time_percentile(0.95):.2f}ms |\n"
    md += f"| p99 | {stats.total.get_response_time_percentile(0.99):.2f}ms |\n\n"
    
    md += f"### Per-Endpoint Stats\n\n"
    md += f"| Endpoint | Requests | Failures | Avg (ms) | p95 (ms) |\n"
    md += f"|----------|----------|----------|----------|----------|\n"
    
    for entry in stats.entries.values():
        p95 = entry.get_response_time_percentile(0.95)
        md += f"| {entry.name} | {entry.num_requests} | {entry.num_failures} | {entry.avg_response_time:.2f} | {p95:.2f} |\n"
    
    # Thresholds check
    md += f"\n### Thresholds\n\n"
    md += f"| Threshold | Status |\n"
    md += f"|-----------|--------|\n"
    
    p95 = stats.total.get_response_time_percentile(0.95)
    p99 = stats.total.get_response_time_percentile(0.99)
    
    p95_ok = p95 < thresholds['p95']
    p99_ok = p99 < thresholds['p99']
    error_ok = failure_rate < thresholds['error_rate']
    
    md += f"| p95 < {thresholds['p95']}ms ({p95:.2f}ms) | {'‚úÖ Passed' if p95_ok else '‚ùå Failed'} |\n"
    md += f"| p99 < {thresholds['p99']}ms ({p99:.2f}ms) | {'‚úÖ Passed' if p99_ok else '‚ùå Failed'} |\n"
    md += f"| Error rate < {thresholds['error_rate']}% ({failure_rate:.2f}%) | {'‚úÖ Passed' if error_ok else '‚ùå Failed'} |\n"
    
    md += f"\n---\n"
    md += f"*Generated by Locust load testing on {datetime.now().isoformat()}*\n"
    
    return md


def generate_json_report(stats: RequestStats, duration: float, thresholds: dict) -> dict:
    """Generate a JSON report from Locust stats."""
    
    total_requests = stats.total.num_requests
    total_failures = stats.total.num_failures
    failure_rate = (total_failures / total_requests * 100) if total_requests > 0 else 0
    
    p95 = stats.total.get_response_time_percentile(0.95)
    p99 = stats.total.get_response_time_percentile(0.99)
    
    return {
        'summary': {
            'total_requests': total_requests,
            'total_failures': total_failures,
            'failure_rate': failure_rate,
            'requests_per_second': total_requests / duration if duration > 0 else 0,
            'duration_seconds': duration,
        },
        'response_time': {
            'avg': stats.total.avg_response_time,
            'min': stats.total.min_response_time,
            'max': stats.total.max_response_time,
            'median': stats.total.get_response_time_percentile(0.50),
            'p90': stats.total.get_response_time_percentile(0.90),
            'p95': p95,
            'p99': p99,
        },
        'thresholds': {
            f'p95_under_{thresholds["p95"]}ms': {'value': p95, 'ok': p95 < thresholds['p95']},
            f'p99_under_{thresholds["p99"]}ms': {'value': p99, 'ok': p99 < thresholds['p99']},
            f'error_rate_under_{thresholds["error_rate"]}pct': {'value': failure_rate, 'ok': failure_rate < thresholds['error_rate']},
        },
        'endpoints': [
            {
                'name': entry.name,
                'requests': entry.num_requests,
                'failures': entry.num_failures,
                'avg_response_time': entry.avg_response_time,
                'p95_response_time': entry.get_response_time_percentile(0.95),
            }
            for entry in stats.entries.values()
        ],
        'timestamp': datetime.now().isoformat(),
    }


if __name__ == "__main__":
    import argparse
    from locust.log import setup_logging
    
    parser = argparse.ArgumentParser(description='Run load tests against http-echo services')
    parser.add_argument('--users', type=int, default=20, help='Number of concurrent users')
    parser.add_argument('--spawn-rate', type=int, default=5, help='Users to spawn per second')
    parser.add_argument('--duration', type=int, default=180, help='Test duration in seconds')
    parser.add_argument('--output-dir', type=str, default='results', help='Output directory for results')
    parser.add_argument('--threshold-p95', type=int, default=500, help='p95 response time threshold in ms')
    parser.add_argument('--threshold-p99', type=int, default=1000, help='p99 response time threshold in ms')
    parser.add_argument('--threshold-error-rate', type=float, default=5.0, help='Error rate threshold in percent')
    parser.add_argument('--urls', type=str, default=None, 
                        help='JSON file containing URLs to test, or inline JSON array. Format: [{"name": "foo", "url": "http://foo.localhost", "expected": "foo"}]')
    args = parser.parse_args()
    
    setup_logging("INFO", None)
    
    # Load URLs from file, inline JSON, or use defaults
    if args.urls:
        if os.path.isfile(args.urls):
            with open(args.urls, 'r') as f:
                TEST_URLS = json.load(f)
        else:
            TEST_URLS = json.loads(args.urls)
    else:
        # Check for environment variables for backward compatibility
        foo_host = os.environ.get('FOO_HOST', 'http://foo.localhost')
        bar_host = os.environ.get('BAR_HOST', 'http://bar.localhost')
        TEST_URLS = [
            {"name": "foo", "url": foo_host, "expected": "foo"},
            {"name": "bar", "url": bar_host, "expected": "bar"},
        ]
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    print(f"==> Starting load test")
    print(f"    URLs to test:")
    for url_config in TEST_URLS:
        print(f"      - {url_config['name']}: {url_config['url']}")
    print(f"    Users: {args.users}")
    print(f"    Spawn Rate: {args.spawn_rate}/s")
    print(f"    Duration: {args.duration}s")
    print(f"    Thresholds: p95<{args.threshold_p95}ms, p99<{args.threshold_p99}ms, errors<{args.threshold_error_rate}%")
    print()
    
    # Create Locust environment
    env = Environment(user_classes=[EchoUser])
    env.create_local_runner()
    
    # Start the test
    start_time = time.time()
    env.runner.start(args.users, spawn_rate=args.spawn_rate)
    
    # Run for specified duration
    try:
        time.sleep(args.duration)
    except KeyboardInterrupt:
        print("\n==> Test interrupted by user")
    
    # Stop the test
    env.runner.stop()
    duration = time.time() - start_time
    
    print(f"\n==> Test completed in {duration:.2f}s")
    print()
    
    # Print overall stats
    print("==> Overall Results Summary")
    print("-" * 60)
    stats = env.runner.stats
    print(f"Total Requests: {stats.total.num_requests}")
    print(f"Failed Requests: {stats.total.num_failures} ({stats.total.fail_ratio * 100:.2f}%)")
    print(f"Requests/sec: {stats.total.num_requests / duration:.2f}")
    print(f"Avg Response Time: {stats.total.avg_response_time:.2f}ms")
    print(f"p95 Response Time: {stats.total.get_response_time_percentile(0.95):.2f}ms")
    print(f"p99 Response Time: {stats.total.get_response_time_percentile(0.99):.2f}ms")
    print("-" * 60)
    
    # Print per-URL stats
    print("\n==> Per-URL Results")
    print("-" * 60)
    for entry in stats.entries.values():
        if entry.num_requests > 0:
            print(f"\n  {entry.name}:")
            print(f"    Requests:      {entry.num_requests}")
            print(f"    Failures:      {entry.num_failures} ({(entry.num_failures / entry.num_requests * 100):.2f}%)")
            print(f"    Avg Response:  {entry.avg_response_time:.2f}ms")
            print(f"    Min Response:  {entry.min_response_time:.2f}ms")
            print(f"    Max Response:  {entry.max_response_time:.2f}ms")
            print(f"    p50 Response:  {entry.get_response_time_percentile(0.50):.2f}ms")
            print(f"    p90 Response:  {entry.get_response_time_percentile(0.90):.2f}ms")
            print(f"    p95 Response:  {entry.get_response_time_percentile(0.95):.2f}ms")
            print(f"    p99 Response:  {entry.get_response_time_percentile(0.99):.2f}ms")
    print("-" * 60)
    
    # Build thresholds dict from args
    thresholds = {
        'p95': args.threshold_p95,
        'p99': args.threshold_p99,
        'error_rate': args.threshold_error_rate,
    }
    
    # Generate and save reports
    json_report = generate_json_report(stats, duration, thresholds)
    md_report = generate_markdown_report(stats, duration, thresholds)
    
    json_path = os.path.join(args.output_dir, 'summary.json')
    md_path = os.path.join(args.output_dir, 'summary.md')
    
    with open(json_path, 'w') as f:
        json.dump(json_report, f, indent=2)
    
    with open(md_path, 'w') as f:
        f.write(md_report)
    
    print(f"\n==> Reports saved to:")
    print(f"    - {json_path}")
    print(f"    - {md_path}")
    
    # Exit with error if thresholds failed
    thresholds_ok = all(t['ok'] for t in json_report['thresholds'].values())
    if not thresholds_ok:
        print("\n‚ö†Ô∏è  Some thresholds failed!")
        exit(1)
    else:
        print("\n‚úÖ All thresholds passed!")
        exit(0)
